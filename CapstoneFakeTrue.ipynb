{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52ffac61-de27-4a13-b96d-f4c752ce20ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter-\n",
      "[nltk_data]     mwohl4/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jupyter-\n",
      "[nltk_data]     mwohl4/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter-\n",
      "[nltk_data]     mwohl4/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jupyter-\n",
      "[nltk_data]     mwohl4/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import text,sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import optim\n",
    "import os\n",
    "#NLP tools\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")   \n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16ef4562-0594-4410-b3ba-583c66fa350f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Donald Trump Sends Out Embarrassing New Year’...\n",
       "1      Drunk Bragging Trump Staffer Started Russian ...\n",
       "2      Sheriff David Clarke Becomes An Internet Joke...\n",
       "3      Trump Is So Obsessed He Even Has Obama’s Name...\n",
       "4      Pope Francis Just Called Out Donald Trump Dur...\n",
       "5      Racist Alabama Cops Brutalize Black Boy While...\n",
       "6      Fresh Off The Golf Course, Trump Lashes Out A...\n",
       "7      Trump Said Some INSANELY Racist Stuff Inside ...\n",
       "8      Former CIA Director Slams Trump Over UN Bully...\n",
       "9      WATCH: Brand-New Pro-Trump Ad Features So Muc...\n",
       "10     Papa John’s Founder Retires, Figures Out Raci...\n",
       "11     WATCH: Paul Ryan Just Told Us He Doesn’t Care...\n",
       "12     Bad News For Trump — Mitch McConnell Says No ...\n",
       "13     WATCH: Lindsey Graham Trashes Media For Portr...\n",
       "14     Heiress To Disney Empire Knows GOP Scammed Us...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overview the data\n",
    "df1 = pd.read_csv('Fake.csv')\n",
    "df2 = pd.read_csv('True.csv')\n",
    "df1['label'] = 0\n",
    "df2['label'] = 1\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.title.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "619b425a-2e4d-4cef-be74-e266987f6096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9ebaa7-d99b-4945-a676-423e8e4fa8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of unwanted columns\n",
    "del df['text']\n",
    "del df['subject']\n",
    "del df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "287c1b08-c574-43a9-8845-4d8adc122608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up title data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"watch:\",'') #gets rid of weird WATCH tag\n",
    "    test = text.replace('\\[[^]]*\\]', '') #gets rid of punctuation\n",
    "    text = text.replace(r\"\\#\",'')\n",
    "    text = text.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\",' ') #non-alphabet and numbers get replaced with space\n",
    "    text = text.replace(\"\\s{2,}\",' ')\n",
    "    return text\n",
    "\n",
    "#do some language processing\n",
    "def remove_stopwords_and_lemmatization(text):\n",
    "    final_text = []\n",
    "    text = nltk.word_tokenize(text)\n",
    "    for word in text:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = WordNetLemmatizer()\n",
    "            word = lemma.lemmatize(word) \n",
    "            final_text.append(word)\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "#Total function\n",
    "def text_munging(text):\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords_and_lemmatization(text)\n",
    "    return text\n",
    "\n",
    "#Apply function on text column\n",
    "df['title_mod'] = df['title'].apply(text_munging)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebacdf21-8160-499a-9f0d-9d9190b4f3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     donald trump sends embarrassing new year ’ eve...\n",
       "1     drunk bragging trump staffer started russian c...\n",
       "2     sheriff david clarke becomes internet joke thr...\n",
       "3     trump obsessed even obama ’ name coded website...\n",
       "4     pope francis called donald trump christmas speech\n",
       "5     racist alabama cop brutalize black boy handcuf...\n",
       "6     fresh golf course , trump lash fbi deputy dire...\n",
       "7     trump said insanely racist stuff inside oval o...\n",
       "8     former cia director slam trump un bullying , o...\n",
       "9     brand-new pro-trump ad feature much * * kissin...\n",
       "10    papa john ’ founder retires , figure racism ba...\n",
       "11    paul ryan told u ’ care struggling family livi...\n",
       "12    bad news trump — mitch mcconnell say repealing...\n",
       "13    lindsey graham trash medium portraying trump ‘...\n",
       "14    heiress disney empire know gop scammed u – shr...\n",
       "Name: title_mod, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title_mod.head(15)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d771dce0-4fa5-4d10-9f46-88b3919fac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on fake news set\n",
    "X_train = df['title_mod']\n",
    "y_train = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62dbc137-c60b-4307-b624-d5842142c675",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n\u001b[1;32m      6\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pad_sequences(tokenized_train, maxlen\u001b[38;5;241m=\u001b[39mmaxlen)\n\u001b[0;32m----> 7\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pad_sequences(tokenized_test, maxlen\u001b[38;5;241m=\u001b[39mmaxlen)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/keras_preprocessing/text.py:281\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtexts_to_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        A list of sequences.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/keras_preprocessing/text.py:309\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    307\u001b[0m     seq \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m vect \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m seq:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/keras_preprocessing/text.py:43\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m# Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 43\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m,):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, unicode):  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "maxlen = 300\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065dbb9-0173-47fa-b156-17c6fa1e6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 100\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=embed_size, input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44da02d5-0e52-48be-ac86-8cdf601cac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "123/123 [==============================] - 125s 1s/step - loss: 0.2231 - accuracy: 0.8864 - val_loss: 0.5051 - val_accuracy: 0.6762\n",
      "Epoch 2/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1965 - accuracy: 0.9020 - val_loss: 0.4629 - val_accuracy: 0.7359\n",
      "Epoch 3/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1824 - accuracy: 0.9110 - val_loss: 0.6325 - val_accuracy: 0.6038\n",
      "Epoch 4/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1653 - accuracy: 0.9229 - val_loss: 0.4900 - val_accuracy: 0.6966\n",
      "Epoch 5/10\n",
      "123/123 [==============================] - 125s 1s/step - loss: 0.1564 - accuracy: 0.9280 - val_loss: 0.6361 - val_accuracy: 0.6093\n",
      "Epoch 6/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1485 - accuracy: 0.9338 - val_loss: 0.5746 - val_accuracy: 0.6562\n",
      "Epoch 7/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1443 - accuracy: 0.9347 - val_loss: 0.5900 - val_accuracy: 0.6562\n",
      "Epoch 8/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1336 - accuracy: 0.9408 - val_loss: 0.5657 - val_accuracy: 0.6733\n",
      "Epoch 9/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1271 - accuracy: 0.9433 - val_loss: 0.9976 - val_accuracy: 0.5581\n",
      "Epoch 10/10\n",
      "123/123 [==============================] - 124s 1s/step - loss: 0.1212 - accuracy: 0.9470 - val_loss: 0.5939 - val_accuracy: 0.7186\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.3, epochs=10, batch_size=batch_size, shuffle=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ed9752b-2b93-4179-9825-209be735e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter title of news article to see if model predicts it is true or fake:  Coronavirus Bioweapon – How China Stole Coronavirus From Canada And Weaponized It\n"
     ]
    }
   ],
   "source": [
    "#time to see what the model says!\n",
    "test_title = input(\"Enter title of news article to see if model predicts it is true or fake: \")\n",
    "#you can find fake news examples here: https://libguides.valenciacollege.edu/c.php?g=612299&p=4251645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2acfb3d7-d416-4927-8010-8b0289e9460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "according to this model, the likelihood that this story is true based on the title is [16.31644]\n"
     ]
    }
   ],
   "source": [
    "test_title_mod = text_munging(test_title)\n",
    "tokenizer.fit_on_texts(test_title_mod)\n",
    "tokenized_predict = tokenizer.texts_to_sequences(test_title_mod)\n",
    "test_title_mod = pad_sequences(tokenized_predict, maxlen=maxlen)\n",
    "perc = model.predict(test_title_mod)[1]*100\n",
    "print('according to this model, the likelihood that this story is true based on the title is ' + str(perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c04505-0b24-48ae-990b-4a9371455769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5637f-7c15-4104-959a-74532e5ef544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
